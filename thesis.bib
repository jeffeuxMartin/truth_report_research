
@article{chorowski_unsupervised_2019,
	title = {Unsupervised speech representation learning using wavenet autoencoders},
	volume = {27},
	number = {12},
	journal = {IEEE/ACM transactions on audio, speech, and language processing},
	author = {Chorowski, Jan and Weiss, Ron J and Bengio, Samy and Van Den Oord, Aäron},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {2041--2053},
	annote = {Comment: Accepted to IEEE TASLP, final version available at http://dx.doi.org/10.1109/TASLP.2019.2938863},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\7LUM82FS\\Chorowski 等。 - 2019 - Unsupervised speech representation learning using .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\UEMS8Q64\\1901.html:text/html},
}

@book{eberhard_ethnologue_2024,
	address = {Dallas, Texas},
	edition = {27th},
	title = {Ethnologue: {Languages} of the {World}},
	url = {https://www.ethnologue.com},
	publisher = {SIL International},
	author = {Eberhard, David M. and Simons, Gary F. and Fennig, Charles D.},
	year = {2024},
	file = {Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\BXHRUATH\\www.ethnologue.com.html:text/html},
}

@misc{lakhotia_generative_2021,
	title = {Generative {Spoken} {Language} {Modeling} from {Raw} {Audio}},
	url = {http://arxiv.org/abs/2102.01192},
	doi = {10.48550/arXiv.2102.01192},
	abstract = {We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.},
	urldate = {2024-05-25},
	publisher = {arXiv},
	author = {Lakhotia, Kushal and Kharitonov, Evgeny and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Adelrahman and Dupoux, Emmanuel},
	month = sep,
	year = {2021},
	note = {arXiv:2102.01192 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\3HQY9RGE\\Lakhotia 等。 - 2021 - Generative Spoken Language Modeling from Raw Audio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\AE3ZGGK3\\2102.html:text/html},
}

@misc{noauthor_textless_2021,
	title = {Textless {NLP}: {Generating} expressive speech from raw audio},
	shorttitle = {Textless {NLP}},
	url = {https://ai.meta.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/},
	abstract = {We’re introducing GSLM, the first language model that breaks free completely of the dependence on text for training. This “textless NLP” approach learns to generate expressive speech using only raw audio recordings as input.},
	language = {en},
	urldate = {2024-05-25},
	month = sep,
	year = {2021},
	file = {Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\Z23UGGPN\\textless-nlp-generating-expressive-speech-from-raw-audio.html:text/html},
}

@inproceedings{ren_speech_2022,
	title = {Speech {Pre}-training with {Acoustic} {Piece}},
	url = {https://www.isca-archive.org/interspeech_2022/ren22_interspeech.html},
	doi = {10.21437/Interspeech.2022-981},
	abstract = {Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens without considering their relations. According to our observation and analysis, the target codes share obvious patterns aligned with phonemized text data. Based on that, we propose to leverage those patterns to better pre-train the model considering the relations among the codes. The patterns we extracted, called “acoustic piece”s, are from the sentence piece result of HuBERT codes. With the acoustic piece as the training signal, we can implicitly bridge the input audio and natural language, which benefits audio-to-text tasks, such as automatic speech recognition (ASR). Simple but effective, our method “HuBERT-AP” significantly outperforms strong baselines on the LibriSpeech ASR task.},
	language = {en},
	urldate = {2024-05-25},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Ren, Shuo and Liu, Shujie and Wu, Yu and Zhou, Long and Wei, Furu},
	month = sep,
	year = {2022},
	pages = {2648--2652},
	file = {Ren 等。 - 2022 - Speech Pre-training with Acoustic Piece.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\GD5LPB7N\\Ren 等。 - 2022 - Speech Pre-training with Acoustic Piece.pdf:application/pdf},
}

@inproceedings{wu_wav2seq_2023,
	address = {Rhodes Island, Greece},
	title = {{Wav2Seq}: {Pre}-{Training} {Speech}-to-{Text} {Encoder}-{Decoder} {Models} {Using} {Pseudo} {Languages}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72816-327-7},
	shorttitle = {{Wav2Seq}},
	url = {https://ieeexplore.ieee.org/document/10096988/},
	doi = {10.1109/ICASSP49357.2023.10096988},
	abstract = {We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new stateof-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.},
	language = {en},
	urldate = {2024-05-25},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Felix and Kim, Kwangyoun and Watanabe, Shinji and Han, Kyu J. and McDonald, Ryan and Weinberger, Kilian Q. and Artzi, Yoav},
	month = jun,
	year = {2023},
	pages = {1--5},
	annote = {ASRU 2023, status : ok
},
	file = {Wu 等。 - 2023 - Wav2Seq Pre-Training Speech-to-Text Encoder-Decod.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\8LLGQKUS\\Wu 等。 - 2023 - Wav2Seq Pre-Training Speech-to-Text Encoder-Decod.pdf:application/pdf},
}

@inproceedings{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	volume = {33},
	shorttitle = {wav2vec 2.0},
	url = {https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2024-05-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	pages = {12449--12460},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\3PZKYXW7\\Baevski 等。 - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learn.pdf:application/pdf},
}

@inproceedings{chang_exploring_2024,
	title = {Exploring {Speech} {Recognition}, {Translation}, and {Understanding} with {Discrete} {Speech} {Units}: {A} {Comparative} {Study}},
	shorttitle = {Exploring {Speech} {Recognition}, {Translation}, and {Understanding} with {Discrete} {Speech} {Units}},
	url = {https://ieeexplore.ieee.org/document/10447929},
	doi = {10.1109/ICASSP48485.2024.10447929},
	abstract = {Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.},
	urldate = {2024-05-26},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chang, Xuankai and Yan, Brian and Choi, Kwanghee and Jung, Jee-Weon and Lu, Yichen and Maiti, Soumi and Sharma, Roshan and Shi, Jiatong and Tian, Jinchuan and Watanabe, Shinji and Fujita, Yuya and Maekaku, Takashi and Guo, Pengcheng and Cheng, Yao-Fei and Denisov, Pavel and Saijo, Kohei and Wang, Hsiu-Hsuan},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Correlation, Discrete units, end-to-end, Redundancy, Self-supervised learning, Speech processing, speech recognition, Speech recognition, speech translation, spoken language understanding, Systematics, Training},
	pages = {11481--11485},
	annote = {ICASSP 2024, status : ok
},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChienChengChen\\Zotero\\storage\\28UTZ5PU\\10447929.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\IGXDWTDB\\Chang 等。 - 2024 - Exploring Speech Recognition, Translation, and Und.pdf:application/pdf},
}

@inproceedings{chen_speech--speech_2023,
	address = {Toronto, Canada},
	title = {Speech-to-{Speech} {Translation} for a {Real}-world {Unwritten} {Language}},
	url = {https://aclanthology.org/2023.findings-acl.307},
	doi = {10.18653/v1/2023.findings-acl.307},
	abstract = {We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automatically mining data from large unlabeled speech datasets, and adopting pseudo-labeling to produce weakly supervised data. On the modeling, we take advantage of recent advances in applying self-supervised discrete representations as target for prediction in S2ST and show the effectiveness of leveraging additional text supervision from Mandarin, a language similar to Hokkien, in model training. Finally, we release an S2ST benchmark set to facilitate future research in this field.},
	urldate = {2024-05-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Peng-Jen and Tran, Kevin and Yang, Yilin and Du, Jingfei and Kao, Justine and Chung, Yu-An and Tomasello, Paden and Duquenne, Paul-Ambroise and Schwenk, Holger and Gong, Hongyu and Inaguma, Hirofumi and Popuri, Sravya and Wang, Changhan and Pino, Juan and Hsu, Wei-Ning and Lee, Ann},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4969--4983},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\CL7VHG2W\\Chen 等。 - 2023 - Speech-to-Speech Translation for a Real-world Unwr.pdf:application/pdf},
}

@inproceedings{lee_self-supervised_2022,
	address = {Seattle, United States},
	title = {Self-supervised {Representation} {Learning} for {Speech} {Processing}},
	url = {https://aclanthology.org/2022.naacl-tutorials.2},
	doi = {10.18653/v1/2022.naacl-tutorials.2},
	abstract = {There is a trend in the machine learning community to adopt self-supervised approaches to pre-train deep networks. Self-supervised representation learning (SSL) utilizes proxy supervised learning tasks, for example, distinguishing parts of the input signal from distractors, or generating masked input segments conditioned on the unmasked ones, to obtain training data from unlabeled corpora. BERT and GPT in NLP and SimCLR and BYOL in CV are famous examples in this direction. These approaches make it possible to use a tremendous amount of unlabeled data available on the web to train large networks and solve complicated tasks. Thus, SSL has the potential to scale up current machine learning technologies, especially for low-resourced, under-represented use cases, and democratize the technologies. Recently self-supervised approaches for speech processing are also gaining popularity. There are several workshops in relevant topics hosted at ICML 2020 (https://icml-sas.gitlab.io/), NeurIPS 2020 (https://neurips-sas-2020.github.io/), and AAAI 2022 (https://aaai-sas-2022.github.io/). However, there is no previous tutorial about a similar topic based on the authors' best knowledge. Due to the growing popularity of SSL, and the shared mission of the areas in bringing speech and language technologies to more use cases with better quality and scaling the technologies for under-represented languages, we propose this tutorial to systematically survey the latest SSL techniques, tools, datasets, and performance achievement in speech processing. The proposed tutorial is highly relevant to the special theme of ACL about language diversity. One of the main focuses of the tutorial is leveraging SSL to reduce the dependence of speech technologies on labeled data, and to scale up the technologies especially for under-represented languages and use cases.},
	urldate = {2024-05-26},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Hung-yi and Mohamed, Abdelrahman and Watanabe, Shinji and Sainath, Tara and Livescu, Karen and Li, Shang-Wen and Yang, Shu-wen and Kirchhoff, Katrin},
	editor = {Ballesteros, Miguel and Tsvetkov, Yulia and Alm, Cecilia O.},
	month = jul,
	year = {2022},
	pages = {8--13},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\SAKG6MIY\\Lee 等。 - 2022 - Self-supervised Representation Learning for Speech.pdf:application/pdf},
}

@article{lakhotia_generative_2021-1,
	title = {On {Generative} {Spoken} {Language} {Modeling} from {Raw} {Audio}},
	volume = {9},
	url = {https://aclanthology.org/2021.tacl-1.79},
	doi = {10.1162/tacl_a_00430},
	abstract = {We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1},
	urldate = {2024-05-26},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and Dupoux, Emmanuel},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {1336--1354},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\JBVNWMSI\\Lakhotia 等。 - 2021 - On Generative Spoken Language Modeling from Raw Au.pdf:application/pdf},
}

@inproceedings{hsu_hubert_2021,
	address = {Toronto, ON, Canada},
	title = {Hubert: {How} {Much} {Can} a {Bad} {Teacher} {Benefit} {ASR} {Pre}-{Training}?},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72817-605-5},
	shorttitle = {Hubert},
	url = {https://ieeexplore.ieee.org/document/9414460/},
	doi = {10.1109/ICASSP39728.2021.9414460},
	abstract = {Compared to vision and language applications, self-supervised pretraining approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to beneﬁt from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.},
	language = {en},
	urldate = {2024-05-26},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Hsu, Wei-Ning and Tsai, Yao-Hung Hubert and Bolte, Benjamin and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	pages = {6533--6537},
	file = {Hsu 等。 - 2021 - Hubert How Much Can a Bad Teacher Benefit ASR Pre.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\695KWE62\\Hsu 等。 - 2021 - Hubert How Much Can a Bad Teacher Benefit ASR Pre.pdf:application/pdf},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\NWMIP87F\\Oord 等。 - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\A7M4JFRM\\1807.html:text/html},
}

@inproceedings{zhao_speech_2023,
	title = {Speech {Enhancement} with {Multi}-granularity {Vector} {Quantization}},
	url = {https://ieeexplore.ieee.org/abstract/document/10317485},
	doi = {10.1109/APSIPAASC58517.2023.10317485},
	abstract = {Neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) has achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown that utilizing a VQ module to discretize noisy speech representation is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.},
	urldate = {2024-05-26},
	booktitle = {2023 {Asia} {Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Zhao, Xiaoying and Zhu, Qiushi and Zhang, Jie and Zhou, Yeping and Liu, Peiqi},
	month = oct,
	year = {2023},
	note = {ISSN: 2640-0103},
	keywords = {Feature extraction, Data mining, Noise reduction, Representation learning, Speech coding, Speech enhancement, Vector quantization},
	pages = {1937--1942},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChienChengChen\\Zotero\\storage\\QE4M75F3\\10317485.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\G5PGLA8H\\Zhao 等。 - 2023 - Speech Enhancement with Multi-granularity Vector Q.pdf:application/pdf},
}

@article{chen_vector_2023,
	title = {A {Vector} {Quantized} {Approach} for {Text} to {Speech} {Synthesis} on {Real}-{World} {Spontaneous} {Speech}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26488},
	doi = {10.1609/aaai.v37i11.26488},
	abstract = {Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.},
	language = {en},
	number = {11},
	urldate = {2024-05-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Li-Wei and Watanabe, Shinji and Rudnicky, Alexander},
	month = jun,
	year = {2023},
	note = {Number: 11},
	keywords = {ML: Applications},
	pages = {12644--12652},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\39M8F3UI\\Chen 等。 - 2023 - A Vector Quantized Approach for Text to Speech Syn.pdf:application/pdf},
}

@inproceedings{chang_exploration_2023,
	title = {Exploration of {Efficient} {End}-to-{End} {ASR} using {Discretized} {Input} from {Self}-{Supervised} {Learning}},
	url = {https://www.isca-archive.org/interspeech_2023/chang23b_interspeech.html},
	doi = {10.21437/Interspeech.2023-2051},
	abstract = {Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and subword modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.},
	language = {en},
	urldate = {2024-05-26},
	booktitle = {{INTERSPEECH} 2023},
	publisher = {ISCA},
	author = {Chang, Xuankai and Yan, Brian and Fujita, Yuya and Maekaku, Takashi and Watanabe, Shinji},
	month = aug,
	year = {2023},
	pages = {1399--1403},
	annote = {INTERSPEECH 2023, status : ok},
	file = {Chang 等。 - 2023 - Exploration of Efficient End-to-End ASR using Disc.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\3MUSCRNE\\Chang 等。 - 2023 - Exploration of Efficient End-to-End ASR using Disc.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/abstract/document/726791},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	urldate = {2024-05-26},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = jan,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChienChengChen\\Zotero\\storage\\2RKSRELY\\726791.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\C9RIWGHQ\\Lecun 等。 - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/},
	number = {3},
	urldate = {2024-05-26},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = oct,
	year = {1959},
	pmid = {14403679},
	pmcid = {PMC1363130},
	pages = {574--591},
	file = {PubMed Central Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\LWQLWXJ7\\Hubel 與 Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2024-05-26},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	note = {Publisher: Springer},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	pages = {115--133},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\EEYQQLYZ\\McCulloch 與 Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {https://doi.apa.org/doi/10.1037/h0042519},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	language = {en},
	number = {6},
	urldate = {2024-05-26},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {Publisher: American Psychological Association},
	pages = {386--408},
	file = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\RBCT9W8K\\Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf;Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\FSXEJW83\\1959-09865-001.html:text/html},
}

@inproceedings{wells_phonetic_2022,
  title   = {Phonetic {Analysis} of {Self}-supervised {Representations} of {English} {Speech}},
  url     = {https://www.isca-archive.org/interspeech_2022/wells22_interspeech.html},
  doi     = {10.21437/Interspeech.2022-10884},
  urldate = {2024-05-29},
  author  = {Wells, Dan and Tang, Hao and Richmond, Korin},
  year    = {2022},
  pages   = {3583--3587},
  annote  = {重要！
             },
  file    = {全文:C\:\\Users\\ChienChengChen\\Zotero\\storage\\EL4MXAG8\\Wells 等。 - 2022 - Phonetic Analysis of Self-supervised Representatio.pdf:application/pdf}
}


@article{funahashi_approximate_1989,
	title = {On the approximate realization of continuous mappings by neural networks},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900038},
	doi = {10.1016/0893-6080(89)90003-8},
	abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.},
	number = {3},
	urldate = {2024-05-29},
	journal = {Neural Networks},
	author = {Funahashi, Ken-Ichi},
	month = jan,
	year = {1989},
	keywords = {Back propagation, Continuous mapping, Hidden layer, Neural network, Output function, Realization, Sigmoid function, Unit},
	pages = {183--192},
	file = {Funahashi - 1989 - On the approximate realization of continuous mappi.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\BRBB8V4B\\Funahashi - 1989 - On the approximate realization of continuous mappi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\ZCUV39I9\\0893608089900038.html:text/html},
}


@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2024-05-29},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {533--536},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\G6L9K9K7\\Rumelhart 等。 - 1986 - Learning representations by back-propagating error.pdf:application/pdf},
}

@incollection{rumelhart_learning_1987,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {978-0-262-29140-8},
	url = {https://ieeexplore.ieee.org/document/6302929},
	abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and McClelland, James L.},
	year = {1987},
	pages = {318--362},
	file = {Learning Internal Representations by Error Propagation | part of Parallel Distributed Processing\: Explorations in the Microstructure of Cognition\: Foundations | MIT Press books | IEEE Xplore:C\:\\Users\\ChienChengChen\\Zotero\\storage\\S63BN74J\\6302929.html:text/html;Learning_Internal_Representations_by_Error_Propagation.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\SS6CZMH4\\Learning_Internal_Representations_by_Error_Propagation.pdf:application/pdf},
}
