@inproceedings{shi2021learning,
  title={Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction},
  author={Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{deseyssel22_interspeech,
  author={Maureen {de Seyssel} and Marvin Lavechin and Yossi Adi and Emmanuel Dupoux and Guillaume Wisniewski},
  title={{Probing phoneme, language and speaker information in unsupervised speech representations}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={1402--1406},
  doi={10.21437/Interspeech.2022-373},
  issn={2308-457X}
}

@inproceedings{abdullah23_interspeech,
  author={Badr M. Abdullah and Mohammed Maqsood Shaik and Bernd Möbius and Dietrich Klakow},
  title={{An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={2883--2887},
  doi={10.21437/Interspeech.2023-2131},
  issn={2308-457X}
}

@article{liu2024dinosr,
  title={Dinosr: Self-distillation and online clustering for self-supervised speech representation learning},
  author={Liu, Alexander H and Chang, Heng-Jui and Auli, Michael and Hsu, Wei-Ning and Glass, Jim},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{huang2023repcodec,
  title={Repcodec: A speech representation codec for speech tokenization},
  author={Huang, Zhichao and Meng, Chutong and Ko, Tom},
  journal={arXiv preprint arXiv:2309.00169},
  year={2023}
}


@INPROCEEDINGS{10097097,
  author={Sicherman, Amitay and Adi, Yossi},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Analysing Discrete Self Supervised Speech Representation For Spoken Language Modeling}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units’ context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units’ clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link.},
  keywords={Visualization;Analytical models;Correlation;Measurement units;Codes;Redundancy;Signal processing;self supervised learning;generative spoken language modeling;textless NLP;speech LM},
  doi={10.1109/ICASSP49357.2023.10097097},
  ISSN={2379-190X},
  month={June},}


@inproceedings{strgar_phoneme_2023,
	title = {Phoneme {Segmentation} {Using} {Self}-{Supervised} {Speech} {Models}},
	url = {https://ieeexplore.ieee.org/abstract/document/10022827},
	doi = {10.1109/SLT54892.2023.10022827},
	abstract = {We apply transfer learning to the task of phoneme segmentation and demonstrate the utility of representations learned in self-supervised pre-training for the task. Our model extends transformer-style encoders with strategically placed convolutions that manipulate features learned in pre-training. Using the TIMIT and Buckeye corpora we train and test the model in the supervised and unsupervised settings. The latter case is accomplished by furnishing a noisy label-set with the predictions of a separate model, it having been trained in an unsupervised fashion. Results indicate our model eclipses previous state-of-the-art performance in both settings and on both datasets. Finally, following observations during published code review and attempts to reproduce past segmentation results, we find a need to disambiguate the definition and implementation of widely-used evaluation metrics. We resolve this ambiguity by delineating two distinct evaluation schemes and describing their nuances.},
	urldate = {2024-06-01},
	booktitle = {2022 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Strgar, Luke and Harwath, David},
	month = jan,
	year = {2023},
	keywords = {Conferences, Convolutional codes, Noise measurement, phonetic boundary detection, Phonetics, Predictive models, self-supervised pre-training, speech segmentation, transfer learning, Transfer learning, Transformers},
	pages = {1067--1073},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\PKQ4D8PM\\Strgar 與 Harwath - 2023 - Phoneme Segmentation Using Self-Supervised Speech .pdf:application/pdf},
}

@inproceedings{tan-bansal-2020-vokenization,
    title = "Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",
    author = "Tan, Hao  and
      Bansal, Mohit",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.162",
    doi = "10.18653/v1/2020.emnlp-main.162",
    pages = "2066--2080",
    abstract = "Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named {``}vokenization{''} that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call {``}vokens{''}). The {``}vokenizer{''} is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.",
}


@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}
@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@misc{maekaku2022speech,
      title={Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021}, 
      author={Takashi Maekaku and Xuankai Chang and Yuya Fujita and Li-Wei Chen and Shinji Watanabe and Alexander Rudnicky},
      year={2022},
      eprint={2107.05899},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@misc{zhang2024speechtokenizer,
      title={SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models}, 
      author={Xin Zhang and Dong Zhang and Shimin Li and Yaqian Zhou and Xipeng Qiu},
      year={2024},
      eprint={2308.16692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lin2022dual,
  title={DUAL: Discrete spoken unit adaptive learning for textless spoken question answering},
  author={Lin, Guan-Ting and Chuang, Yung-Sung and Chung, Ho-Lam and Yang, Shu-wen and Chen, Hsuan-Jui and Dong, Shuyan and Li, Shang-Wen and Mohamed, Abdelrahman and Lee, Hung-yi and Lee, Lin-shan},
  journal={arXiv preprint arXiv:2203.04911},
  year={2022}
}

@misc{rivière2020unsupervised,
      title={Unsupervised pretraining transfers well across languages}, 
      author={Morgane Rivière and Armand Joulin and Pierre-Emmanuel Mazaré and Emmanuel Dupoux},
      year={2020},
      eprint={2002.02848},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@article{baevski2019vq,
  title={vq-wav2vec: Self-supervised learning of discrete speech representations},
  author={Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  journal={arXiv preprint arXiv:1910.05453},
  year={2019}
}




@inproceedings{schneider2019wav2vec,
  author={Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli},
  title={{wav2vec: Unsupervised Pre-Training for Speech Recognition}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={3465--3469},
  doi={10.21437/Interspeech.2019-1873},
  issn={2308-457X}
}


@article{hsu_hubert_2021-2,
	title = {Hubert: {Self}-supervised speech representation learning by masked prediction of hidden units},
	volume = {29},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {3451--3460},
}


@inproceedings{ravanelli_multi-task_2020,
	title = {Multi-{Task} {Self}-{Supervised} {Learning} for {Robust} {Speech} {Recognition}},
	doi = {10.1109/ICASSP40776.2020.9053569},
	abstract = {Despite the growing interest in unsupervised learning, extracting meaningful knowledge from unlabelled audio remains an open challenge. To take a step in this direction, we recently proposed a problem-agnostic speech encoder (PASE), that combines a convolutional encoder followed by multiple neural networks, called workers, tasked to solve self-supervised problems (i.e., ones that do not require manual annotations as ground truth). PASE was shown to capture relevant speech information, including speaker voice-print and phonemes. This paper proposes PASE+, an improved version of PASE for robust speech recognition in noisy and reverberant environments. To this end, we employ an online speech distortion module, that contaminates the input signals with a variety of random disturbances. We then propose a revised encoder that better learns short- and long-term speech dynamics with an efficient combination of recurrent and convolutional networks. Finally, we refine the set of workers used in self-supervision to encourage better cooperation. Results on TIMIT, DIRHA and CHiME-5 show that PASE+ significantly outperforms both the previous version of PASE as well as common acoustic features. Interestingly, PASE+ learns transferable representations suitable for highly mismatched acoustic conditions.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Speech processing, speech recognition, Speech recognition, self-supervised learning, Task analysis, Acoustic distortion, Acoustics, Convolution, Noise measurement},
	pages = {6989--6993},
}

@inproceedings{chung_generative_2020,
	title = {Generative {Pre}-{Training} for {Speech} with {Autoregressive} {Predictive} {Coding}},
	doi = {10.1109/ICASSP40776.2020.9054438},
	abstract = {Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chung, Yu-An and Glass, James},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {autoregressive modeling, Data models, pre-training, Predictive coding, representation learning, self-supervised learning, Speech coding, Speech recognition, Task analysis, Training, transfer learning, Transfer learning},
	pages = {3497--3501},
}


@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2024-05-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}


@article{radford_language_nodate,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{t_tera_2021,
	title = {{TERA}: {Self}-{Supervised} {Learning} of {Transformer} {Encoder} {Representation} for {Speech}},
	copyright = {2329-9290 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.},
	shorttitle = {{TERA}},
	url = {https://dl.acm.org/doi/10.1109/TASLP.2021.3095662},
	doi = {10.1109/TASLP.2021.3095662},
	abstract = {We introduce a self-supervised speech pre-training method called TERA, which stands
for Transformer Encoder Representations from Alteration. Recent approaches often learn
by using a single auxiliary task like contrastive prediction, autoregressive ...},
	language = {EN},
	urldate = {2024-05-30},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {T, LiuAndy and LiShang-Wen and LeeHung-yi},
	year = {2021},
	note = {Publisher: IEEE},
	file = {送出的版本:C\:\\Users\\ChienChengChen\\Zotero\\storage\\DN229NUL\\T 等。 - 2021 - TERA Self-Supervised Learning of Transformer Enco.pdf:application/pdf;Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\EXVUC8IV\\TASLP.2021.html:text/html},
}

    @misc{liu_mockingjay_2019,
	title = {Mockingjay: {Unsupervised} {Speech} {Representation} {Learning} with {Deep} {Bidirectional} {Transformer} {Encoders}},
	shorttitle = {Mockingjay},
	url = {https://arxiv.org/abs/1910.12638v2},
	abstract = {We present Mockingjay as a new speech representation learning approach, where bidirectional Transformer encoders are pre-trained on a large amount of unlabeled speech. Previous speech representation methods learn through conditioning on past frames and predicting information about future frames. Whereas Mockingjay is designed to predict the current frame through jointly conditioning on both past and future contexts. The Mockingjay representation improves performance for a wide range of downstream tasks, including phoneme classification, speaker recognition, and sentiment classification on spoken content, while outperforming other approaches. Mockingjay is empirically powerful and can be fine-tuned with downstream models, with only 2 epochs we further improve performance dramatically. In a low resource setting with only 0.1\% of labeled data, we outperform the result of Mel-features that uses all 100\% labeled data.},
	language = {en},
	urldate = {2024-05-30},
	journal = {arXiv.org},
	author = {Liu, Andy T. and Yang, Shu-wen and Chi, Po-Han and Hsu, Po-chun and Lee, Hung-yi},
	month = oct,
	year = {2019},
	doi = {10.1109/ICASSP40776.2020.9054458},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\K25WEQYZ\\Liu 等。 - 2019 - Mockingjay Unsupervised Speech Representation Lea.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-05-30},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\NJWNWKCD\\Devlin 等。 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}


@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://aclanthology.org/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2024-05-30},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {2227--2237},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\82CY5Y9L\\Peters 等。 - 2018 - Deep Contextualized Word Representations.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\JY2B9ND7\\Mikolov 等。 - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf},
}


@article{chorowski_unsupervised_2019,
	title = {Unsupervised speech representation learning using wavenet autoencoders},
	volume = {27},
	number = {12},
	journal = {IEEE/ACM transactions on audio, speech, and language processing},
	author = {Chorowski, Jan and Weiss, Ron J and Bengio, Samy and Van Den Oord, Aäron},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {2041--2053},
	annote = {Comment: Accepted to IEEE TASLP, final version available at http://dx.doi.org/10.1109/TASLP.2019.2938863},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\7LUM82FS\\Chorowski 等。 - 2019 - Unsupervised speech representation learning using .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\UEMS8Q64\\1901.html:text/html},
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}


@inproceedings{cho-etal-2014-properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    editor = "Wu, Dekai  and
      Carpuat, Marine  and
      Carreras, Xavier  and
      Vecchi, Eva Maria",
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@misc{sutskever2014sequence,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@book{eberhard_ethnologue_2024,
	address = {Dallas, Texas},
	edition = {27th},
	title = {Ethnologue: {Languages} of the {World}},
	url = {https://www.ethnologue.com},
	publisher = {SIL International},
	author = {Eberhard, David M. and Simons, Gary F. and Fennig, Charles D.},
	year = {2024},
	file = {Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\BXHRUATH\\www.ethnologue.com.html:text/html},
}

@misc{lakhotia_generative_2021,
	title = {Generative {Spoken} {Language} {Modeling} from {Raw} {Audio}},
	url = {http://arxiv.org/abs/2102.01192},
	doi = {10.48550/arXiv.2102.01192},
	abstract = {We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.},
	urldate = {2024-05-25},
	publisher = {arXiv},
	author = {Lakhotia, Kushal and Kharitonov, Evgeny and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Adelrahman and Dupoux, Emmanuel},
	month = sep,
	year = {2021},
	note = {arXiv:2102.01192 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\3HQY9RGE\\Lakhotia 等。 - 2021 - Generative Spoken Language Modeling from Raw Audio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\AE3ZGGK3\\2102.html:text/html},
}

@misc{noauthor_textless_2021,
	title = {Textless {NLP}: {Generating} expressive speech from raw audio},
	shorttitle = {Textless {NLP}},
	url = {https://ai.meta.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/},
	abstract = {We’re introducing GSLM, the first language model that breaks free completely of the dependence on text for training. This “textless NLP” approach learns to generate expressive speech using only raw audio recordings as input.},
	language = {en},
	urldate = {2024-05-25},
	month = sep,
	year = {2021},
	file = {Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\Z23UGGPN\\textless-nlp-generating-expressive-speech-from-raw-audio.html:text/html},
}

@inproceedings{ren_speech_2022,
	title = {Speech {Pre}-training with {Acoustic} {Piece}},
	url = {https://www.isca-archive.org/interspeech_2022/ren22_interspeech.html},
	doi = {10.21437/Interspeech.2022-981},
	abstract = {Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens without considering their relations. According to our observation and analysis, the target codes share obvious patterns aligned with phonemized text data. Based on that, we propose to leverage those patterns to better pre-train the model considering the relations among the codes. The patterns we extracted, called “acoustic piece”s, are from the sentence piece result of HuBERT codes. With the acoustic piece as the training signal, we can implicitly bridge the input audio and natural language, which benefits audio-to-text tasks, such as automatic speech recognition (ASR). Simple but effective, our method “HuBERT-AP” significantly outperforms strong baselines on the LibriSpeech ASR task.},
	language = {en},
	urldate = {2024-05-25},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Ren, Shuo and Liu, Shujie and Wu, Yu and Zhou, Long and Wei, Furu},
	month = sep,
	year = {2022},
	pages = {2648--2652},
	file = {Ren 等。 - 2022 - Speech Pre-training with Acoustic Piece.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\GD5LPB7N\\Ren 等。 - 2022 - Speech Pre-training with Acoustic Piece.pdf:application/pdf},
}

@inproceedings{wu_wav2seq_2023,
	address = {Rhodes Island, Greece},
	title = {{Wav2Seq}: {Pre}-{Training} {Speech}-to-{Text} {Encoder}-{Decoder} {Models} {Using} {Pseudo} {Languages}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72816-327-7},
	shorttitle = {{Wav2Seq}},
	url = {https://ieeexplore.ieee.org/document/10096988/},
	doi = {10.1109/ICASSP49357.2023.10096988},
	abstract = {We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new stateof-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.},
	language = {en},
	urldate = {2024-05-25},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Felix and Kim, Kwangyoun and Watanabe, Shinji and Han, Kyu J. and McDonald, Ryan and Weinberger, Kilian Q. and Artzi, Yoav},
	month = jun,
	year = {2023},
	pages = {1--5},
	annote = {ASRU 2023, status : ok
},
	file = {Wu 等。 - 2023 - Wav2Seq Pre-Training Speech-to-Text Encoder-Decod.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\8LLGQKUS\\Wu 等。 - 2023 - Wav2Seq Pre-Training Speech-to-Text Encoder-Decod.pdf:application/pdf},
}

@inproceedings{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	volume = {33},
	shorttitle = {wav2vec 2.0},
	url = {https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2024-05-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
	year = {2020},
	pages = {12449--12460},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\3PZKYXW7\\Baevski 等。 - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learn.pdf:application/pdf},
}

@inproceedings{chang_exploring_2024,
	title = {Exploring {Speech} {Recognition}, {Translation}, and {Understanding} with {Discrete} {Speech} {Units}: {A} {Comparative} {Study}},
	shorttitle = {Exploring {Speech} {Recognition}, {Translation}, and {Understanding} with {Discrete} {Speech} {Units}},
	url = {https://ieeexplore.ieee.org/document/10447929},
	doi = {10.1109/ICASSP48485.2024.10447929},
	abstract = {Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.},
	urldate = {2024-05-26},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chang, Xuankai and Yan, Brian and Choi, Kwanghee and Jung, Jee-Weon and Lu, Yichen and Maiti, Soumi and Sharma, Roshan and Shi, Jiatong and Tian, Jinchuan and Watanabe, Shinji and Fujita, Yuya and Maekaku, Takashi and Guo, Pengcheng and Cheng, Yao-Fei and Denisov, Pavel and Saijo, Kohei and Wang, Hsiu-Hsuan},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Correlation, Discrete units, end-to-end, Redundancy, Self-supervised learning, Speech processing, speech recognition, Speech recognition, speech translation, spoken language understanding, Systematics, Training},
	pages = {11481--11485},
	annote = {ICASSP 2024, status : ok
},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChienChengChen\\Zotero\\storage\\28UTZ5PU\\10447929.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\IGXDWTDB\\Chang 等。 - 2024 - Exploring Speech Recognition, Translation, and Und.pdf:application/pdf},
}

@inproceedings{chen_speech--speech_2023,
	address = {Toronto, Canada},
	title = {Speech-to-{Speech} {Translation} for a {Real}-world {Unwritten} {Language}},
	url = {https://aclanthology.org/2023.findings-acl.307},
	doi = {10.18653/v1/2023.findings-acl.307},
	abstract = {We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automatically mining data from large unlabeled speech datasets, and adopting pseudo-labeling to produce weakly supervised data. On the modeling, we take advantage of recent advances in applying self-supervised discrete representations as target for prediction in S2ST and show the effectiveness of leveraging additional text supervision from Mandarin, a language similar to Hokkien, in model training. Finally, we release an S2ST benchmark set to facilitate future research in this field.},
	urldate = {2024-05-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Peng-Jen and Tran, Kevin and Yang, Yilin and Du, Jingfei and Kao, Justine and Chung, Yu-An and Tomasello, Paden and Duquenne, Paul-Ambroise and Schwenk, Holger and Gong, Hongyu and Inaguma, Hirofumi and Popuri, Sravya and Wang, Changhan and Pino, Juan and Hsu, Wei-Ning and Lee, Ann},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4969--4983},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\CL7VHG2W\\Chen 等。 - 2023 - Speech-to-Speech Translation for a Real-world Unwr.pdf:application/pdf},
}

@inproceedings{lee_self-supervised_2022,
	address = {Seattle, United States},
	title = {Self-supervised {Representation} {Learning} for {Speech} {Processing}},
	url = {https://aclanthology.org/2022.naacl-tutorials.2},
	doi = {10.18653/v1/2022.naacl-tutorials.2},
	abstract = {There is a trend in the machine learning community to adopt self-supervised approaches to pre-train deep networks. Self-supervised representation learning (SSL) utilizes proxy supervised learning tasks, for example, distinguishing parts of the input signal from distractors, or generating masked input segments conditioned on the unmasked ones, to obtain training data from unlabeled corpora. BERT and GPT in NLP and SimCLR and BYOL in CV are famous examples in this direction. These approaches make it possible to use a tremendous amount of unlabeled data available on the web to train large networks and solve complicated tasks. Thus, SSL has the potential to scale up current machine learning technologies, especially for low-resourced, under-represented use cases, and democratize the technologies. Recently self-supervised approaches for speech processing are also gaining popularity. There are several workshops in relevant topics hosted at ICML 2020 (https://icml-sas.gitlab.io/), NeurIPS 2020 (https://neurips-sas-2020.github.io/), and AAAI 2022 (https://aaai-sas-2022.github.io/). However, there is no previous tutorial about a similar topic based on the authors' best knowledge. Due to the growing popularity of SSL, and the shared mission of the areas in bringing speech and language technologies to more use cases with better quality and scaling the technologies for under-represented languages, we propose this tutorial to systematically survey the latest SSL techniques, tools, datasets, and performance achievement in speech processing. The proposed tutorial is highly relevant to the special theme of ACL about language diversity. One of the main focuses of the tutorial is leveraging SSL to reduce the dependence of speech technologies on labeled data, and to scale up the technologies especially for under-represented languages and use cases.},
	urldate = {2024-05-26},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Hung-yi and Mohamed, Abdelrahman and Watanabe, Shinji and Sainath, Tara and Livescu, Karen and Li, Shang-Wen and Yang, Shu-wen and Kirchhoff, Katrin},
	editor = {Ballesteros, Miguel and Tsvetkov, Yulia and Alm, Cecilia O.},
	month = jul,
	year = {2022},
	pages = {8--13},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\SAKG6MIY\\Lee 等。 - 2022 - Self-supervised Representation Learning for Speech.pdf:application/pdf},
}

@article{lakhotia_generative_2021-1,
	title = {On {Generative} {Spoken} {Language} {Modeling} from {Raw} {Audio}},
	volume = {9},
	url = {https://aclanthology.org/2021.tacl-1.79},
	doi = {10.1162/tacl_a_00430},
	abstract = {We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1},
	urldate = {2024-05-26},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and Dupoux, Emmanuel},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {1336--1354},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\JBVNWMSI\\Lakhotia 等。 - 2021 - On Generative Spoken Language Modeling from Raw Au.pdf:application/pdf},
}

@inproceedings{hsu_hubert_2021,
	address = {Toronto, ON, Canada},
	title = {Hubert: {How} {Much} {Can} a {Bad} {Teacher} {Benefit} {ASR} {Pre}-{Training}?},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72817-605-5},
	shorttitle = {Hubert},
	url = {https://ieeexplore.ieee.org/document/9414460/},
	doi = {10.1109/ICASSP39728.2021.9414460},
	abstract = {Compared to vision and language applications, self-supervised pretraining approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to beneﬁt from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.},
	language = {en},
	urldate = {2024-05-26},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Hsu, Wei-Ning and Tsai, Yao-Hung Hubert and Bolte, Benjamin and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	pages = {6533--6537},
	file = {Hsu 等。 - 2021 - Hubert How Much Can a Bad Teacher Benefit ASR Pre.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\695KWE62\\Hsu 等。 - 2021 - Hubert How Much Can a Bad Teacher Benefit ASR Pre.pdf:application/pdf},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\NWMIP87F\\Oord 等。 - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\A7M4JFRM\\1807.html:text/html},
}

@inproceedings{zhao_speech_2023,
	title = {Speech {Enhancement} with {Multi}-granularity {Vector} {Quantization}},
	url = {https://ieeexplore.ieee.org/abstract/document/10317485},
	doi = {10.1109/APSIPAASC58517.2023.10317485},
	abstract = {Neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) has achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown that utilizing a VQ module to discretize noisy speech representation is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.},
	urldate = {2024-05-26},
	booktitle = {2023 {Asia} {Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Zhao, Xiaoying and Zhu, Qiushi and Zhang, Jie and Zhou, Yeping and Liu, Peiqi},
	month = oct,
	year = {2023},
	note = {ISSN: 2640-0103},
	keywords = {Feature extraction, Data mining, Noise reduction, Representation learning, Speech coding, Speech enhancement, Vector quantization},
	pages = {1937--1942},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChienChengChen\\Zotero\\storage\\QE4M75F3\\10317485.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\G5PGLA8H\\Zhao 等。 - 2023 - Speech Enhancement with Multi-granularity Vector Q.pdf:application/pdf},
}

@article{chen_vector_2023,
	title = {A {Vector} {Quantized} {Approach} for {Text} to {Speech} {Synthesis} on {Real}-{World} {Spontaneous} {Speech}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26488},
	doi = {10.1609/aaai.v37i11.26488},
	abstract = {Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.},
	language = {en},
	number = {11},
	urldate = {2024-05-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Li-Wei and Watanabe, Shinji and Rudnicky, Alexander},
	month = jun,
	year = {2023},
	note = {Number: 11},
	keywords = {ML: Applications},
	pages = {12644--12652},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\39M8F3UI\\Chen 等。 - 2023 - A Vector Quantized Approach for Text to Speech Syn.pdf:application/pdf},
}

@inproceedings{chang_exploration_2023,
	title = {Exploration of {Efficient} {End}-to-{End} {ASR} using {Discretized} {Input} from {Self}-{Supervised} {Learning}},
	url = {https://www.isca-archive.org/interspeech_2023/chang23b_interspeech.html},
	doi = {10.21437/Interspeech.2023-2051},
	abstract = {Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and subword modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.},
	language = {en},
	urldate = {2024-05-26},
	booktitle = {{INTERSPEECH} 2023},
	publisher = {ISCA},
	author = {Chang, Xuankai and Yan, Brian and Fujita, Yuya and Maekaku, Takashi and Watanabe, Shinji},
	month = aug,
	year = {2023},
	pages = {1399--1403},
	annote = {INTERSPEECH 2023, status : ok},
	file = {Chang 等。 - 2023 - Exploration of Efficient End-to-End ASR using Disc.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\3MUSCRNE\\Chang 等。 - 2023 - Exploration of Efficient End-to-End ASR using Disc.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/abstract/document/726791},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	urldate = {2024-05-26},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = jan,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChienChengChen\\Zotero\\storage\\2RKSRELY\\726791.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\C9RIWGHQ\\Lecun 等。 - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/},
	number = {3},
	urldate = {2024-05-26},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = oct,
	year = {1959},
	pmid = {14403679},
	pmcid = {PMC1363130},
	pages = {574--591},
	file = {PubMed Central Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\LWQLWXJ7\\Hubel 與 Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2024-05-26},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	note = {Publisher: Springer},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	pages = {115--133},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\EEYQQLYZ\\McCulloch 與 Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {https://doi.apa.org/doi/10.1037/h0042519},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	language = {en},
	number = {6},
	urldate = {2024-05-26},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {Publisher: American Psychological Association},
	pages = {386--408},
	file = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\RBCT9W8K\\Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf;Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\FSXEJW83\\1959-09865-001.html:text/html},
}

@inproceedings{wells_phonetic_2022,
  title   = {Phonetic {Analysis} of {Self}-supervised {Representations} of {English} {Speech}},
  url     = {https://www.isca-archive.org/interspeech_2022/wells22_interspeech.html},
  doi     = {10.21437/Interspeech.2022-10884},
  urldate = {2024-05-29},
  author  = {Wells, Dan and Tang, Hao and Richmond, Korin},
  year    = {2022},
  pages   = {3583--3587},
  annote  = {重要！
             },
  file    = {全文:C\:\\Users\\ChienChengChen\\Zotero\\storage\\EL4MXAG8\\Wells 等。 - 2022 - Phonetic Analysis of Self-supervised Representatio.pdf:application/pdf}
}


@article{funahashi_approximate_1989,
	title = {On the approximate realization of continuous mappings by neural networks},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900038},
	doi = {10.1016/0893-6080(89)90003-8},
	abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.},
	number = {3},
	urldate = {2024-05-29},
	journal = {Neural Networks},
	author = {Funahashi, Ken-Ichi},
	month = jan,
	year = {1989},
	keywords = {Back propagation, Continuous mapping, Hidden layer, Neural network, Output function, Realization, Sigmoid function, Unit},
	pages = {183--192},
	file = {Funahashi - 1989 - On the approximate realization of continuous mappi.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\BRBB8V4B\\Funahashi - 1989 - On the approximate realization of continuous mappi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\ChienChengChen\\Zotero\\storage\\ZCUV39I9\\0893608089900038.html:text/html},
}


@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2024-05-29},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {533--536},
	file = {Full Text PDF:C\:\\Users\\ChienChengChen\\Zotero\\storage\\G6L9K9K7\\Rumelhart 等。 - 1986 - Learning representations by back-propagating error.pdf:application/pdf},
}

@incollection{rumelhart_learning_1987,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {978-0-262-29140-8},
	url = {https://ieeexplore.ieee.org/document/6302929},
	abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and McClelland, James L.},
	year = {1987},
	pages = {318--362},
	file = {Learning Internal Representations by Error Propagation | part of Parallel Distributed Processing\: Explorations in the Microstructure of Cognition\: Foundations | MIT Press books | IEEE Xplore:C\:\\Users\\ChienChengChen\\Zotero\\storage\\S63BN74J\\6302929.html:text/html;Learning_Internal_Representations_by_Error_Propagation.pdf:C\:\\Users\\ChienChengChen\\Zotero\\storage\\SS6CZMH4\\Learning_Internal_Representations_by_Error_Propagation.pdf:application/pdf},
}
