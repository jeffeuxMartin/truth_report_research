\chapter{多個語音離散表徵與音位的關係}
\newcommand{\myhline}{\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}}

\newcommand{\draftbegin}{\centerline{\textcolor{magenta}{\textbf{=======================以下是草稿！=======================}}}}
\newcommand{\drafttermi}{\centerline{\textcolor{blue}{\textbf{=======================以上是草稿！=======================}}}}



\section{動機}
　　
如前一章所述，由於單一離散單元所代表的僅為 10 或 20 毫秒的語音訊號，而作為一種類似文字的語音內容表示，每一個音位往往都對應超過一個以上的離散單元。因此，基於使得離散單元序列可以不論在長度和對應的語音訊號兩方面都能更接近於文字，因此從自然語言處理的分詞演算法（Tokenization）得到靈感，本章節將嘗試將這些演算法應用於離散單元的序列之上，並應用上一章節的分析方式，比對將多個離散單元重新分組形成符記之後，是否可以在保有「完全從語音訊號獲得」的同時，得到更接近於音位的序列。
　　
\draftbegin
\input{4-2-related}

\input{4-3-tokenization}
\drafttermi

\section{衡量方式}
　　
本章節沿用上一章節 LibriSpeech 資料集的 train-clean-100 訓練子集，以及相同的分析數據以進行比對。由於與上一章節的差異僅在分詞方法的引入，因此會多一個序列長短縮短的比例；另外，有一個操控變因是分詞方法與詞表的大小。為了統一比較，以下分析結果的分詞方法統一使用 SentencePiece 套件中的 BPE 演算法，並比較詞表大小 500、2000、10000 三種大小的差異。

\draftbegin
\input{4-5-results}
\newline
\drafttermi

\section{本章總結}
　　
藉由分詞演算法的引入，我們可以發現在序列長度相對縮短的前提下，音位的純度卻也獲得了提升，足以證明分詞演算法的引入，可以幫助離散單元考量多於一個音框的語音資訊，建構於精細的音框之上，找出更接近人類解讀語音最小單位資訊。期望以此發現，可以使得語音語言模型建立時，模型在處理語音語料庫時，能夠以更接近文字的序列長度與資訊進行訓練，獲得更接近文字模型的效果。
