
\section{\% !TeX root = ../thesis.tex}

\chapter{Ch2. 背景知識}

\section{2.1 類神經網路}

\section{((Like this \textbackslash{}cite\{726791, baevski2020wav2vec\} and fade out.))}

\section{2.1.1 簡介}

((深層類神經網路是 McColloh 在 1950 年提出的計算模型，其概念取法自連結主意學派，旨在用計算模型模擬生物神經連結的現象。類神經網路最基本的單元為一個神經元（neuron），來自 1960 年 Rosenblatt 提出的感知器（Perceptron）模型，旨在根據輸入訊號給出的線性分類器，其數學是可以表達成：))

類神經網路是 McCulloch 和 Pitts  \href{https://www.zotero.org/google-docs/?IG2P0K}{(McCulloch \& Pitts, 1943)} 在 1943 年提出的計算模型，旨在模仿生物神經的連結，並能透過演算法的最佳化，擬和我們想要的函數以實現特定的應用功能。由於其彈性，目前已成為人工智慧發展的主流。

類神經網路最基本的單元是「神經元」，其本質為一個線性分類器，會接受一串數字作為輸入並計算出一個數字作為輸出，可用下列數學運算式描述：

$$y=\textbackslash{}sigma(w\^\top x + b) $$

其中 $x$ 是 $N$ 個輸入的數字，可描述為一個 $N$ 維的向量；$w$ 為該神經元對每個輸入值給予的權重，再對加權平均後的結果加上偏差值 $b$ 後，經過激發函數（Activation Function）$\sigma$ 的非線性轉換後最後得到輸出。常用的激發函數包含 ReLU、sigmoid 和 tanh 等等。

藉由結合好幾個神經元的運算，基於 Universal Approx. Thm. ，在數學理論上我們可以近乎模擬一切函數，這樣的機器學習模型即為 Perceptron \_\_cite\_\_。然而單純增多神經元數目仍無法解決如 XOR 等分類問題，於是爾後 MLP 的概念被 \_\_\_ 提出，即結合多層感知器，在輸入與輸出之間加入隱藏層（hidden layer）以對運算數據進行表徵空間上的轉換，可以更好的拓展類神經網路的適用範圍，解決更加複雜的現實問題。此種透過加深類神經網路隱藏層形成的計算模型便被稱為深層類神經網路（deep neural network）。

然而單純擁有一個可以表達複雜函數的模型是不夠解決工程應用問題的，為了增加函數擬和（fit）的效率，\_\_\_ 在 \_\_\_\_ 年提出了 backpropagation 的演算法，旨在藉由計算輸出層與目標函數之間的誤差，透過最佳化演算法計算出梯度後，經由隱藏層反向往輸入層對於整個類神經網路進行修正，便能配合 GPU 大量平行運算的能力，很好的從資料中找尋出我們想要的函數。這樣透過深層類神經網路，從資料輸入與輸出之間尋找函數的機器學習演算法，就稱之為深層學習。由於深層學習的 scalibility 與泛用性，不論在圖像、語音、文字等多個模態，深層類神經網路都已經獲得了廣泛應用。

然而根據資料特性的不同， 並不是所有的資料都單純適用這樣輸入與輸出向量直接對應的模式，因此類神經網路又發展出不同的架構以適應資料本身的特性。前述的類神經網路由於運算過程單純是從輸入層經由多層感知器直接進行矩陣運算完成函數的模擬，因此被稱之為「前饋式類神經網路」。與此直接計算不同，在輸入與輸出之間調整連接關係，可以得到 CNN、RNN 與 Transformer 等架構。由於這些架構在語音與文本處理上已是主流選擇，接下來分別介紹：

\section{2.1.2 CNN}

卷積式類神經網路（convolutional neural network）為 1998 年由楊氏（LeCun）提出 \textbackslash{}cite\{726791\}，旨在利用訊號處理上卷積（convolution）的運算模擬人類視覺皮質感知 \textbackslash{}cite\{hubel1959receptive\} 的特性，利用其移動不變性（shift-invariance）來捕捉二維影像中的局部（local）特徵，以便於後續的類神經網路可以對輸入的資料進行更整體而全面的判斷。

有別於圖像中經常是以 pixel 的 RGB 亮度進行卷積運算，在語音中 CNN 處理的對象除了直接是空氣壓力波形的物理訊號以外，為了更方便機器模型判斷語音訊號的內容，透過聲學知識得到的聲學特徵或深層學習得出的語音表徵也經常是語音處理中卷積層運算的對象。然而不論是何種輸入，有別於影像的二維資料，語音訊號的資訊是被呈現在時間軸的維度上，因此通常使用一維的卷積式類神經網路，以模仿人耳聽覺對時變訊號的窗框（window）處理過程，讓模型可以觀察到輸入語音在不同解析度（resolution）上的資訊，例如本研究特別著重的音位（phoneme）等。

// text embedding 先不寫

// speech acoustic features 拖到後面去寫

\section{2.1.3 RNN}

有別於運算過程由輸入往輸出單向的 FFN 和 CNN，為了處理有記憶和狀態的資料，特別是會隨時間變化的序列資訊，在語音和文字的機器學習中，會將輸出訊號重新接回輸入層的 RNN 是一個相當符合語言特性的選擇。RNN 以每個時間點（timestep）為考慮對象，在每一步會對輸入層的向量進行運算後，不但將此結果算出一個輸出向量，還會得到另外一些數據保留作內部狀態，表示此前經歷過所有序列資料的記憶。常用的 RNN 的類型有 LSTM 和 GRU，這兩種 RNN 的示意圖與運算式如下：

// 放運算式和示意圖

此類類神經網路通常會以下列介紹的序列至序列的形式被用在如語音辨識、語音合成或機器翻譯等和語言密切相關的任務中。

\section{2.1.4 Seq2seq}

由於許多以語言為主的資料經常以兩個序列互相配對的形式呈現，因此專門用以處理此類資料的模型被特別稱為序列至序列模型。此類模型一般的架構是由一個編碼器（encoder）和一個解碼器（decoder）構成，旨在模擬輸入與輸出序列之間的變化與相依關係（dependency）。

此類模型一般有兩種模式：

其一是每個時間點都取得一個輸出的向量，用在輸入與輸出等長的任務之中，此模式又被稱為 token classification。

但更常見的狀況是，輸入與輸出兩者序列長度並不總是相同，此時典型的作法是，讓編碼器將輸入序列在每個時間點一一與模型進行運算，藉由內部表徵（latent representation）的調整對整個輸入序列進行編碼，完成後將最後一個時間點的表徵作為整個序列的代表，此表徵向量會被稱為「語境向量（context vector）」，接著被傳遞給解碼器依時序生成輸出訊號的序列。

\section{2.1.5 Attention}

然而由於 RNN 本身需要編碼和解碼的資訊量是整個序列，對時間點距離比較遠的輸入容易被遺忘，也就是難以處理長期相依性（long-term dependency）的問題。為解決這種困境，Luong 等人提出了「專注機制（attention mechanism）」，讓解碼器除了依據語境向量的資訊以外，還可以對輸入序列的不同時間點分配權重，在生成輸出序列時重新從輸入序列中得到所需的訊息。

專注機制一般涉及三個向量之間的運算：query、key 和 value，其運算式如下：

(KQV 運算)

具有專注機制的序列至序列模型又被稱為 AED，透過專注機制的引入，大大改善了如語音辨識、機器翻譯等任務的效能。

\section{2.1.6 Transformer}

儘管 RNN 本身善於處理時序資料，然而它難以平行化的架構限制卻大大束縛了其在訓練和推理（inference）時的效率。由 attention 獲取靈感，2017 年瓦氏（Vaswani） 等人在 \_\_cite\_\_ 提出了一種完全由專注機制構成，不需依賴遞迴運算的序列至序列模型，用以解決最經典的機器翻譯任務。

\subsection{2.1.6.1 Transformer 架構}

Transformer 一樣沿用了 Attention 的 KQV 三組向量的邏輯，以 positional encoding 對序列中每個位置的時間點進行編碼，取代原先在 RNN 模型需要一一運算的過程，在實行平行計算的同時也能考慮到資料在不同時間點出現的效應。其整體架構如下：

(tfm 的圖)

(講多頭專注、KQV、FFN 那些)

由於轉換器不需對每個時間點一一運算，使其得以實現高度平行化的優勢，類神經網路得以透過專注機制的幫助同時進行序列資料的大量訓練，這種 scalibility 因而在自然語言和語音處理都獲得了巨大的進展，近乎取代了原先 RNN 的應用場景，近年甚至被電腦視覺的研究者推廣應用在圖像類的資料上( \_\_vit\_\_) ，足以展現此種模型架構的彈性與泛用性，是目前最前沿人工智慧的主流架構。

除了模型架構，機器學習中不可或缺的另一大 component 即是對資料的編碼過程。如何更有效率的讓機器可以理解、處理和輸出，是機器學習乃至深層學習的一大課題。面對捉摸不定、抽象且變化萬千的人類語言，語音和文字處理如何對資料去蕪存菁，表徵學習更是重中之重。

\section{2.2 表徵（representation）學習與自監督式學習（SSL）}

\section{2.2.1 聲學特徵}

為了讓機器可以理解輸入的資料，表徵學習是機器學習中不可或缺的一部分。

在語音處理中，在過往機器運算能力還沒有那麼強大的時候，人們基於聲學原理，使用 MFCC 為處理的對象。在文字中則通常使用 TF-IDF

(這邊寫 mfcc 的介紹)

\section{2.2.2 表徵學習}

後來 Mikolov 提出了 word2vec 的做法，使用 distributed representation 對文字的單詞（word）進行編碼，透過大量的文本單詞之間的 co-occurence 去找出每個單詞最適合的語義表徵。其後 ELMo 提出了 contextualized embedding 的想法，更細緻的在單詞本身之外也嘗試對句子脈絡的語義進行詞嵌入（word embedding）編碼。

\section{2.2.3 自監督學習}

爾後在 Transformer 模型被提出後，BERT (cite BERT) 被提出，從大量的文本與自專注機制之中，工程師們便可不借助人為的標記，透過預先設定的 pretext tasks 的引導，使得模型可以自己從大量文本中更細緻、更 contextualized 的自行找出語義關係，並在許多 NLP 的任務上獲得了 SoTA 的成績。自此「self-supervised learning (SSL)」的概念大行其道，這種以 pretext tasks 代替標註資料本身，從大量的未標註資料中利用資料本身結構進行學習的模式成為主流。由於其學習對象是發掘自大量的資料本身，可以更好的利用 NN 的泛化（generalization）能力去找出什麼樣的資訊對於人們日常應用任務中是重要的並予以保留。

有鑑於文字處理方面的成功有許多的語音處理學者便嘗試將類似的模式套用在語音訊號之上自此提出了很多的語音基石模型而這些藉由語音基石模型得出來的語音表徵，也在很多任務上被證明可以超越傳統上使用的 fbank、 MFCC 等工程特徵，因為大量的語音資料庫本身可以幫助模型去萃取出更適用於各種語音任務上的向量表徵。

依照這些語音自監督模型的學習模式，大致可以分為重建式、預測式與對比式模型，以下分別按照這三類模式介紹這些語音基石模型：

(這邊接下來直接看以前碩論怎麼分。宏毅老師的 review paper 再說）\}

\subsection{2.2.3.1 重建式學習}

此類模型在文字處理以 BERT 為代表，BERT 本身屬於遮罩語言模型（masked language model，MLM）。在語音中以 Mockingjay、TERA 為主要採取此模式的基石模型。

\subsection{2.2.3.2 預測式學習}

此類模型在文字處理以 GPT 為典型，不同於 BERT 是任意對資料進行擾動作為預訓練任務，這類模型的目標即是單純的自迴歸（auto-regressive），可以用以下式子來表達其訓練 objective：

\$$ (寫那個 y = p(x|x<t) 什麼的式子）\$$

在語音中以 APC 為代表。

（是不是有一個寫法把前兩個都當成預測，只是一個是重建一個是自迴歸？）

\subsection{2.2.3.3 對比式學習}

這類模型以 CPC 為主。在電腦視覺有 BYOL 等等。

\section{2.2.3 向量量化（vector quantization）}

基於向量本身容易受噪聲擾動而導致訓練不穩定，因此為了穩定訓練，向量量化（vector quantization）的技巧變常常為機器學習，尤其是語音這邊所使用。例如 vq-wav2vec 與其後的 wav2vec 2.0 就使用了這樣的技巧，HuBERT 本身對語音表徵進行 KMeans clustering 也是一個向量量化的手段。（是不是應該寫一下 KMeans 是什麼？）

（補說一下什麼是 discrete unit）

\section{2.2.4 離散單元與無文字（textless）架構}

由於 HuBERT 本身的成功（好像要寫理由？），其後 Meta 提出了完全基於 HuBERT unit 的抽取方式，完全只依賴語音而不依靠文字標註的「無文字（textless）」架構被提出，其代表作為 GSLM。（好像也要提一下 speech-to-speech 翻譯嗎？）

無文字目前在 QA (cite 實驗室的 DUAL) 跟語音到語音翻譯（cite 並描述臺語翻英語翻譯）獲得了很好的成功。自此這類「離散單元（discrete unit）」被視為一項類似文字卻不需要真的依賴人類文字標記的語音表徵，其最大優勢為儲存的 bit rate 低與可以套用 NLP 文字的「語言模型」之訓練模式。

然而，雖然在系統與應用 task 上獲得了很大的成功，但 unit 本身是否已經真的很好的可以替代文字，或能夠多少的幫助 spoken language model 的訓練與建立，仍然是目前本領域探討的焦點議題。有鑑於此，本論文基於語言知識，從最接近文字但又跟語音訊號最密切相關的 phoneme 開始探討，期望對 unit 本身究竟能夠帶給我們什麼特徵、如何幫助後續應用進行進一步的研究。

\section{2.3 本章節總結}

本章節先是對作為 building block 的類神經網路進行了基本原理的介紹，其後對本論文研究的核心──「representation」與「discrete unit」的發展演進與歷史進行了簡單的梳理。此後兩章節就會緊扣著這些基石模型得到的離散特徵，將其與尤其是 phoneme 這類語音學標記之間的統計關係進行更進一步的分析。

（Ch 3 好像要講一下為什麼不做連續特徵 \& 為什麼要以 phoneme 為客體了 T\_T）

 